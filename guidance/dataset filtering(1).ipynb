{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0844b9",
   "metadata": {},
   "source": [
    "# Generating Personalized Emails with AWS Bedrock and AWS Personalize\n",
    "\n",
    "\n",
    "In this notebook, we're building a product recommendation engine that can provide personalized recommendations to users based on their past ratings and reviews. We will be working with pandas to utilize dataset filtering and then export the data to DynamoDB and S3 for future processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23310a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dccf8f",
   "metadata": {},
   "source": [
    "***Retrieve Bucket name*** \n",
    "\n",
    "Replace the stack_name with the stack name associated with your CloudFormation template. Remove the <> after entering the stack name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_client = boto3.client('cloudformation')\n",
    "\n",
    "stack_name = '<YOUR-STACK-NAME>'\n",
    "response = cf_client.describe_stacks(StackName=stack_name)\n",
    "stack_outputs = response['Stacks'][0]['Outputs']\n",
    "\n",
    "bucket_name = None\n",
    "for output in stack_outputs:\n",
    "    if output['OutputKey'] == 'S3Bucket': \n",
    "        bucket_name = output['OutputValue']\n",
    "        break\n",
    "        \n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860140c",
   "metadata": {},
   "source": [
    "***Creating the Product Review Dataset***\n",
    "\n",
    "Taking the initial amazon.csv file, we will be filtering out the respect columns of data to create a product data set. This would include fields like timestamp, product id, product name, and description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the product review dataset filtering out respective columns\n",
    "\n",
    "\n",
    "review_data_key = 'amazon_data.csv'\n",
    "data_s3_location1 = \"s3://{}/{}\".format(bucket_name, review_data_key) \n",
    "product_data = pd.read_csv(data_s3_location1, encoding='latin1')\n",
    "\n",
    "product_data = product_data.drop('discounted_price', axis=1)\n",
    "product_data = product_data.drop('actual_price', axis=1)\n",
    "product_data = product_data.drop('discount_percentage', axis=1)\n",
    "product_data = product_data.drop('user_id', axis=1)\n",
    "product_data = product_data.drop('user_name', axis=1)\n",
    "product_data = product_data.drop('review_id', axis=1)\n",
    "product_data = product_data.drop('review_title', axis=1)\n",
    "product_data = product_data.drop('review_content', axis=1)\n",
    "product_data = product_data.drop('img_link', axis=1)\n",
    "product_data = product_data.drop('product_link', axis=1)\n",
    "product_data = product_data.drop('rating', axis=1)\n",
    "product_data = product_data.drop('age', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "product_data.rename(columns={'timestamp': 'CREATION_TIMESTAMP', 'product_id': 'ITEM_ID', 'product_name': 'PRODUCT_NAME', 'category': 'CATEGORY','rating_count': 'RATING_COUNT','about_product': 'DESCRIPTION'}, inplace=True)\n",
    "product_data['CREATION_TIMESTAMP'] = pd.to_datetime(product_data['CREATION_TIMESTAMP']).astype(int) // 10**9  # Convert to Unix timestamp (seconds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "product_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b84b4",
   "metadata": {},
   "source": [
    "## Uploading the data to DynamoDB\n",
    "\n",
    "DynamoDB is a highly scalable, low-latency, and fully managed NoSQL database service which is well-suited for applications that require fast and consistent performance, such as personalization recommendation engines. DynamoDB is reliable and durable choice for storing and retrieving large volumes of data. We will be utilizing on demand capacity to account for unpredictable traffic patterns from our dataset.\n",
    "\n",
    "\n",
    "Using the boto client, we create a table with a schema where ITEM_ID is the primary key. After ensuring the table is ready, we convert the rating_count column in the product_data DataFrame to strings and iterate through the DataFrame, inserting each row as an item into the DynamoDB table. The creation of the table may take up to ***one minute*** to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the product data to DynamoDB\n",
    "\n",
    "\n",
    "session = boto3.Session()\n",
    "dynamodb = session.resource('dynamodb')\n",
    "\n",
    "table_name = 'productdata'\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    table = dynamodb.Table(table_name)\n",
    "    table.load()\n",
    "    print(f\"Table {table_name} already exists.\")\n",
    "except dynamodb.meta.client.exceptions.ResourceNotFoundException:\n",
    "    # Define the table schema\n",
    "    table = dynamodb.create_table(\n",
    "        TableName=table_name,\n",
    "        KeySchema=[\n",
    "            {\n",
    "                'AttributeName': 'ITEM_ID',\n",
    "                'KeyType': 'HASH'  \n",
    "            }\n",
    "        ],\n",
    "        AttributeDefinitions=[\n",
    "            {\n",
    "                'AttributeName': 'ITEM_ID',\n",
    "                'AttributeType': 'S'\n",
    "            }\n",
    "        ],\n",
    "        BillingMode='PAY_PER_REQUEST'\n",
    "    )\n",
    "    table.meta.client.get_waiter('table_exists').wait(TableName=table_name)\n",
    "    print(f\"Table {table_name} created successfully.\")\n",
    "\n",
    "\n",
    "    \n",
    "product_data['RATING_COUNT'] = product_data['RATING_COUNT'].astype(str)\n",
    "\n",
    "# Define the DynamoDB table\n",
    "table = dynamodb.Table(table_name)\n",
    "\n",
    "# Insert data into DynamoDB\n",
    "for index, row in product_data.iterrows():\n",
    "    item = {\n",
    "        'TIMESTAMP': row['CREATION_TIMESTAMP'],\n",
    "        'ITEM_ID': row['ITEM_ID'],\n",
    "        'PRODUCT_NAME': row['PRODUCT_NAME'],\n",
    "        'CATEGORY': row['CATEGORY'],\n",
    "        'RATING_COUNT': row['RATING_COUNT'],\n",
    "        'DESCRIPTION': row['DESCRIPTION']\n",
    "    }\n",
    "    \n",
    "    table.put_item(Item=item)\n",
    "\n",
    "print(\"Data inserted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b86cb6",
   "metadata": {},
   "source": [
    "***Create the user dataset***\n",
    "\n",
    "We then load the user review data from an Amazon S3 bucket into a DataFrame filtering out certain columns and renaming some columns for consistency. Convert the TIMESTAMP column to a Unix timestamp in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d19b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the user review dataset filtering out respective columns\n",
    "\n",
    "user_data_key = 'amazon_data.csv'\n",
    "data_s3_location1 = \"s3://{}/{}\".format(bucket_name, user_data_key)  # S3 URL\n",
    "user_data = pd.read_csv(data_s3_location1, encoding='latin1')\n",
    "\n",
    "user_data = user_data.drop('rating_count', axis=1)\n",
    "user_data = user_data.drop('category', axis=1)\n",
    "user_data = user_data.drop('about_product', axis=1)\n",
    "user_data = user_data.drop('img_link', axis=1)\n",
    "user_data = user_data.drop('product_link', axis=1)\n",
    "user_data = user_data.drop('discounted_price', axis=1)\n",
    "user_data = user_data.drop('actual_price', axis=1)\n",
    "user_data = user_data.drop('discount_percentage', axis=1)\n",
    "\n",
    "\n",
    "user_data.rename(columns={'timestamp': 'TIMESTAMP', 'product_id': 'ITEM_ID', 'product_name':'PRODUCT_NAME','rating':'RATING','user_id': 'USER_ID', 'age': 'AGE','user_name':'USERNAME', 'review_id': 'REVIEW_ID','review_title':'REVIEW_TITLE','review_content':'REVIEW_CONTENT'}, inplace=True)\n",
    "\n",
    "user_data['TIMESTAMP'] = pd.to_datetime(user_data['TIMESTAMP']).astype(int) // 10**9  # Convert to Unix timestamp (seconds)\n",
    "\n",
    "\n",
    "user_data.info()\n",
    "user_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a1cd0",
   "metadata": {},
   "source": [
    "***Write back user data to DynamoDB***\n",
    "\n",
    "\n",
    "Using the boto client, we create a table with a schema where review_id as the primary key. After ensuring the table is ready, we convert the rating and review_content columns in the user_data DataFrame to strings.Iterate through the DataFrame, inserting each row as an item into the DynamoDB table. The creation of the table may take up to ***one minute*** to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97892a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "\n",
    "# Get the DynamoDB resource\n",
    "dynamodb = session.resource('dynamodb')\n",
    "\n",
    "# Define the table\n",
    "table_name = 'userdata'\n",
    "\n",
    "# Check if the table exists, if not, create it\n",
    "try:\n",
    "    table = dynamodb.Table(table_name)\n",
    "    table.load()\n",
    "    print(f\"Table {table_name} already exists.\")\n",
    "except dynamodb.meta.client.exceptions.ResourceNotFoundException:\n",
    "    # Define the table schema\n",
    "    table = dynamodb.create_table(\n",
    "        TableName=table_name,\n",
    "        KeySchema=[\n",
    "            {\n",
    "                'AttributeName': 'REVIEW_ID',\n",
    "                'KeyType': 'HASH' \n",
    "            }\n",
    "        ],\n",
    "        AttributeDefinitions=[\n",
    "            {\n",
    "                'AttributeName': 'REVIEW_ID',\n",
    "                'AttributeType': 'S'\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        BillingMode='PAY_PER_REQUEST'\n",
    "        \n",
    "    )\n",
    "\n",
    "    table.meta.client.get_waiter('table_exists').wait(TableName=table_name)\n",
    "    print(f\"Table {table_name} created successfully.\")\n",
    "\n",
    "table = dynamodb.Table(table_name)\n",
    "user_data['RATING'] = user_data['RATING'].astype(str)\n",
    "user_data['REVIEW_CONTENT'] = user_data['REVIEW_CONTENT'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "for index, row in user_data.iterrows():\n",
    "    item = {\n",
    "        'TIMESTAMP': row['TIMESTAMP'],\n",
    "        'AGE': row['AGE'],\n",
    "        'PRODUCT_ID': row['ITEM_ID'],\n",
    "        'PRODUCT_NAME': row['PRODUCT_NAME'],\n",
    "        'REVIEW_ID': row['REVIEW_ID'],\n",
    "        'REVIEW_TITLE': row['REVIEW_TITLE'],\n",
    "        'USER_ID': row['USER_ID'],\n",
    "        'USERNAME': row['USERNAME'],\n",
    "        'REVIEW_CONTENT': row['REVIEW_CONTENT']\n",
    "    }\n",
    "    \n",
    "    table.put_item(Item=item)\n",
    "\n",
    "print(\"Data inserted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d100718",
   "metadata": {},
   "source": [
    "***Creating the interaction dataset***\n",
    "\n",
    "We are performing sentiment analysis on product reviews using Amazon Comprehend in preparation for training a personalized recommendation model with Amazon Personalize. The sentiment analysis provides valuable insights into the users' opinions and attitudes towards specific products, which can be leveraged by the Amazon Personalize to generate more accurate and relevant recommendations.\n",
    "\n",
    "- Enriching User Interactions: The sentiment analysis adds an extra layer of information to the user interaction data. By analyzing the sentiment of reviews, we can better understand the user's experience and preferences with different products. Positive sentiments indicate satisfaction, while negative sentiments may suggest dissatisfaction or areas for improvement.\n",
    "\n",
    "- Improving Recommendation Relevance: Amazon Personalize uses various data sources, including user interactions, to train its recommendation models. By incorporating sentiment information, the model can learn not only what products a user interacted with but also their subjective opinions about those products. This additional context can help the model better understand user preferences and provide more relevant recommendations.\n",
    "\n",
    "This may take up to a **minute** to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Amazon Comprehend client\n",
    "comprehend = boto3.client('comprehend', region_name='us-west-2')\n",
    "\n",
    "def chunk_text(text, chunk_size=4800):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        while end < len(text) and ord(text[end]) > 127 and ord(text[end]) < 192:\n",
    "            end -= 1\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def get_comprehend_sentiment(text, max_length=5000):\n",
    "    if not text:\n",
    "        return None, None\n",
    "    \n",
    "    if len(text.encode('utf-8')) > max_length:\n",
    "        text = text[:max_length//2]  \n",
    "    chunks = chunk_text(text)\n",
    "    sentiments = []\n",
    "    sentiment_scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if len(chunk.encode('utf-8')) > max_length:\n",
    "            raise ValueError(f\"Chunk size exceeds {max_length} bytes limit.\")\n",
    "        response = comprehend.detect_sentiment(Text=chunk, LanguageCode='en')\n",
    "        sentiment = response['Sentiment'].upper()\n",
    "        sentiment_score = response['SentimentScore'][sentiment.capitalize()]\n",
    "        sentiments.append(sentiment)\n",
    "        sentiment_scores.append(sentiment_score)\n",
    "    \n",
    "    avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else None\n",
    "    return sentiments[0], avg_sentiment_score\n",
    "\n",
    "# Assuming user_data is your DataFrame containing the user data\n",
    "processed_data = user_data.copy()\n",
    "processed_data['RATING'] = pd.to_numeric(processed_data['RATING'], errors='coerce')\n",
    "\n",
    "# Apply sentiment analysis to the 'REVIEW_CONTENT' column\n",
    "processed_data[['SENTIMENT', 'SENTIMENTSCORE']] = processed_data['REVIEW_CONTENT'].apply(lambda x: pd.Series(get_comprehend_sentiment(x)))\n",
    "\n",
    "# Add the EVENT_TYPE column\n",
    "processed_data['EVENT_TYPE'] = None\n",
    "processed_data.loc[processed_data['RATING'] < 4.0, 'EVENT_TYPE'] = 'read'\n",
    "processed_data.loc[processed_data['RATING'] > 4.0, 'EVENT_TYPE'] = 'click'\n",
    "\n",
    "\n",
    "# Filter rows that have an EVENT_TYPE assigned\n",
    "interactions_df = processed_data[processed_data['EVENT_TYPE'].notna()]\n",
    "interactions_df = interactions_df[['TIMESTAMP', 'USERNAME', 'USER_ID','ITEM_ID', 'PRODUCT_NAME', 'EVENT_TYPE', 'SENTIMENT', 'SENTIMENTSCORE']]\n",
    "interactions_df['SENTIMENTSCORE'] = interactions_df['SENTIMENTSCORE'].astype(str)\n",
    "interactions_df['USER_ID'] = interactions_df['USER_ID'].astype(str)\n",
    "\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(interactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71598c08",
   "metadata": {},
   "source": [
    "## Read datasets into S3\n",
    "\n",
    "Read the interaction,product,and user dataset in the initialized S3 bucket. Reading data into Amazon S3 is a common practice when utilizing Amazon Personalize because it requires input data to be stored in an S3 bucket before it can be used to train and deploy models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read interaction data into S3\n",
    "\n",
    "from io import StringIO\n",
    "interactions_filename = \"interactions.csv\"\n",
    "\n",
    "interactions_df.info()\n",
    "print(interactions_df.columns)\n",
    "\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "interactions_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.put_object(Bucket= bucket_name, Key='interactions.csv', Body = csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaca241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read product data into S3\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "product_data.to_csv(csv_buffer, index=False)\n",
    "\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.put_object(Bucket= bucket_name, Key='product_data.csv', Body = csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10063884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user data into S3\n",
    "\n",
    "user_data = user_data.drop('REVIEW_CONTENT', axis=1)\n",
    "csv_buffer = StringIO()\n",
    "user_data.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.put_object(Bucket=bucket_name, Key='user_data.csv', Body = csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6499185",
   "metadata": {},
   "source": [
    "***Store the variables for future use***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f70d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store product_data\n",
    "%store user_data\n",
    "%store interactions_df\n",
    "%store interactions_filename\n",
    "%store bucket_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0844b9",
   "metadata": {},
   "source": [
    "***Generating Personalized Emails with AWS Bedrock and AWS Personalize***\n",
    "\n",
    "\n",
    "In this notebook, we will be working to implement product recommendations for user's based on past ratings and reviews. We will then incorporate AWS Bedrock to product marketing emails which will then be sent out to the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23310a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dccf8f",
   "metadata": {},
   "source": [
    "***Creating the Product Review Dataset***\n",
    "\n",
    "Taking the initial amazon.csv file, we will be filtering out the respect columns of data to create a product data set. This would include fields like timestamp, product id, product name, and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the product review dataset filtering out respective columns\n",
    "\n",
    "\n",
    "bucket_name = 'personalizeproductreviewdata'\n",
    "review_data_key = 'amazon_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "data_s3_location1 = \"s3://{}/{}\".format(bucket_name, review_data_key)  # S3 URL\n",
    "product_data = pd.read_csv(data_s3_location1)\n",
    "\n",
    "\n",
    "product_data = product_data.drop('discounted_price', axis=1)\n",
    "product_data = product_data.drop('actual_price', axis=1)\n",
    "product_data = product_data.drop('discount_percentage', axis=1)\n",
    "product_data = product_data.drop('user_id', axis=1)\n",
    "product_data = product_data.drop('user_name', axis=1)\n",
    "product_data = product_data.drop('review_id', axis=1)\n",
    "product_data = product_data.drop('review_title', axis=1)\n",
    "product_data = product_data.drop('review_content', axis=1)\n",
    "product_data = product_data.drop('img_link', axis=1)\n",
    "product_data = product_data.drop('product_link', axis=1)\n",
    "product_data = product_data.drop('rating', axis=1)\n",
    "product_data = product_data.drop('age', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "product_data.rename(columns={'timestamp': 'CREATION_TIMESTAMP', 'product_id': 'ITEM_ID', 'about_product': 'DESCRIPTION'}, inplace=True)\n",
    "product_data['CREATION_TIMESTAMP'] = pd.to_datetime(product_data['CREATION_TIMESTAMP']).astype(int) // 10**9  # Convert to Unix timestamp (seconds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "product_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b84b4",
   "metadata": {},
   "source": [
    "***Uploading the data to DynamoDB***\n",
    "\n",
    "Using the boto client, we create a table with a schema where ITEM_ID is the primary key. After ensuring the table is ready, we convert the rating_count column in the product_data DataFrame to strings and iterate through the DataFrame, inserting each row as an item into the DynamoDB table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the product data to DynamoDB\n",
    "\n",
    "\n",
    "session = boto3.Session()\n",
    "dynamodb = session.resource('dynamodb')\n",
    "\n",
    "table_name = 'productdata'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    table = dynamodb.Table(table_name)\n",
    "    table.load()\n",
    "    print(f\"Table {table_name} already exists.\")\n",
    "except dynamodb.meta.client.exceptions.ResourceNotFoundException:\n",
    "    # Define the table schema\n",
    "    table = dynamodb.create_table(\n",
    "        TableName=table_name,\n",
    "        KeySchema=[\n",
    "            {\n",
    "                'AttributeName': 'ITEM_ID',\n",
    "                'KeyType': 'HASH'  \n",
    "            }\n",
    "        ],\n",
    "        AttributeDefinitions=[\n",
    "            {\n",
    "                'AttributeName': 'ITEM_ID',\n",
    "                'AttributeType': 'S'\n",
    "            }\n",
    "        ],\n",
    "        BillingMode='PAY_PER_REQUEST'\n",
    "    )\n",
    "    table.meta.client.get_waiter('table_exists').wait(TableName=table_name)\n",
    "    print(f\"Table {table_name} created successfully.\")\n",
    "\n",
    "\n",
    "    \n",
    "product_data['rating_count'] = product_data['rating_count'].astype(str)\n",
    "\n",
    "# Define the DynamoDB table\n",
    "table = dynamodb.Table(table_name)\n",
    "\n",
    "# Insert data into DynamoDB\n",
    "for index, row in product_data.iterrows():\n",
    "    item = {\n",
    "        'timestamp': row['CREATION_TIMESTAMP'],\n",
    "        'ITEM_ID': row['ITEM_ID'],\n",
    "        'product_name': row['product_name'],\n",
    "        'category': row['category'],\n",
    "        'rating_count': row['rating_count'],\n",
    "        'description': row['DESCRIPTION']\n",
    "    }\n",
    "    \n",
    "    table.put_item(Item=item)\n",
    "\n",
    "print(\"Data inserted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b86cb6",
   "metadata": {},
   "source": [
    "***Create the user dataset***\n",
    "\n",
    "We then load the user review data from an Amazon S3 bucket into a DataFrame filtering out certain columns and renaming some columns for consistency. Convert the TIMESTAMP column to a Unix timestamp in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d19b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the user review dataset filtering out respective columns\n",
    "\n",
    "bucket_name = 'personalizeproductreviewdata'\n",
    "user_data_key = 'amazon_data.csv'\n",
    "data_s3_location1 = \"s3://{}/{}\".format(bucket_name, user_data_key)  # S3 URL\n",
    "user_data = pd.read_csv(data_s3_location1)\n",
    "\n",
    "\n",
    "user_data = user_data.drop('rating_count', axis=1)\n",
    "user_data = user_data.drop('category', axis=1)\n",
    "user_data = user_data.drop('about_product', axis=1)\n",
    "user_data = user_data.drop('img_link', axis=1)\n",
    "user_data = user_data.drop('product_link', axis=1)\n",
    "user_data = user_data.drop('discounted_price', axis=1)\n",
    "user_data = user_data.drop('actual_price', axis=1)\n",
    "user_data = user_data.drop('discount_percentage', axis=1)\n",
    "\n",
    "\n",
    "user_data.rename(columns={'timestamp': 'TIMESTAMP', 'user_id': 'USER_ID', 'age': 'AGE'}, inplace=True)\n",
    "\n",
    "user_data['TIMESTAMP'] = pd.to_datetime(user_data['TIMESTAMP']).astype(int) // 10**9  # Convert to Unix timestamp (seconds)\n",
    "#user_data['rating'] = str(user_data['rating'])\n",
    "\n",
    "\n",
    "\n",
    "user_data.info()\n",
    "user_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a1cd0",
   "metadata": {},
   "source": [
    "***Write back user data to DynamoDB***\n",
    "\n",
    "\n",
    "Using the boto client, we create a table with a schema where review_id as the primary key. After ensuring the table is ready, we convert the rating and review_content columns in the user_data DataFrame to strings.Iterate through the DataFrame, inserting each row as an item into the DynamoDB table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97892a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read back the user data to DynamoDB\n",
    "\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the DynamoDB resource\n",
    "dynamodb = session.resource('dynamodb')\n",
    "\n",
    "# Define the table\n",
    "table_name = 'userdata'\n",
    "\n",
    "# Check if the table exists, if not, create it\n",
    "try:\n",
    "    table = dynamodb.Table(table_name)\n",
    "    table.load()\n",
    "    print(f\"Table {table_name} already exists.\")\n",
    "except dynamodb.meta.client.exceptions.ResourceNotFoundException:\n",
    "    # Define the table schema\n",
    "    table = dynamodb.create_table(\n",
    "        TableName=table_name,\n",
    "        KeySchema=[\n",
    "            {\n",
    "                'AttributeName': 'review_id',\n",
    "                'KeyType': 'HASH' \n",
    "            }\n",
    "        ],\n",
    "        AttributeDefinitions=[\n",
    "            {\n",
    "                'AttributeName': 'review_id',\n",
    "                'AttributeType': 'S'\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        BillingMode='PAY_PER_REQUEST'\n",
    "        \n",
    "    )\n",
    "\n",
    "    table.meta.client.get_waiter('table_exists').wait(TableName=table_name)\n",
    "    print(f\"Table {table_name} created successfully.\")\n",
    "\n",
    "table = dynamodb.Table(table_name)\n",
    "user_data['rating'] = user_data['rating'].astype(str)\n",
    "user_data['review_content'] = user_data['review_content'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "for index, row in user_data.iterrows():\n",
    "    item = {\n",
    "        'timestamp': row['TIMESTAMP'],\n",
    "        'age': row['AGE'],\n",
    "        'product_id': row['product_id'],\n",
    "        'review_id': row['review_id'],\n",
    "        'review_title': row['review_title'],\n",
    "        'rating': row['rating'],\n",
    "        'user_id': row['USER_ID'],\n",
    "        'user_name': row['user_name'],\n",
    "        'review_content': row['review_content']\n",
    "    }\n",
    "    \n",
    "    table.put_item(Item=item)\n",
    "\n",
    "print(\"Data inserted successfully.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d100718",
   "metadata": {},
   "source": [
    "***Creating the interaction dataset***\n",
    "\n",
    "We perform sentiment analysis on user reviews using Amazon Comprehend, breaking down long reviews into manageable chunks for analysis. We then calculates the average sentiment score for each review and assigns an event type based on the rating, creating a new DataFrame with relevant columns for user interactions, sentiment, and event types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on review using Amazon Comprehend\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "comprehend = boto3.client('comprehend', region_name='us-west-2')\n",
    "\n",
    "def chunk_text(text, chunk_size=4000):\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def get_comprehend_sentiment(text):\n",
    "    if not text:\n",
    "        return None, None\n",
    "    chunks = chunk_text(text)\n",
    "    sentiments = []\n",
    "    sentiment_scores = []\n",
    "    for chunk in chunks:\n",
    "        response = comprehend.detect_sentiment(Text=chunk, LanguageCode='en')\n",
    "        sentiment = response['Sentiment']\n",
    "        sentiment_score = response['SentimentScore'][sentiment.capitalize()]\n",
    "        sentiments.append(sentiment)\n",
    "        sentiment_scores.append(sentiment_score)\n",
    "    avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
    "    return sentiments[0], avg_sentiment_score\n",
    "\n",
    "processed_data = user_data.copy()\n",
    "processed_data['rating'] = pd.to_numeric(processed_data['rating'], errors='coerce')\n",
    "\n",
    "# Apply sentiment analysis to the 'review_content' column\n",
    "processed_data[['Sentiment', 'SentimentScore']] = processed_data['review_content'].apply(lambda x: pd.Series(get_comprehend_sentiment(x)))\n",
    "\n",
    "# Add the EVENT_TYPE column\n",
    "processed_data['EVENT_TYPE'] = None\n",
    "processed_data.loc[processed_data['rating'] > 4.0, 'EVENT_TYPE'] = 'read'\n",
    "processed_data.loc[processed_data['rating'] > 3.0, 'EVENT_TYPE'] = 'click'\n",
    "\n",
    "# Filter rows that have an EVENT_TYPE assigned\n",
    "interactions_df = processed_data[processed_data['EVENT_TYPE'].notna()]\n",
    "\n",
    "# Select relevant columns and rename them\n",
    "interactions_df = interactions_df[['TIMESTAMP', 'user_name', 'product_id', 'product_name', 'EVENT_TYPE', 'Sentiment', 'SentimentScore']]\n",
    "interactions_df.rename(columns={\n",
    "    'timestamp': 'TIMESTAMP',\n",
    "    'user_name': 'USER_ID',\n",
    "    'product_id': 'ITEM_ID',\n",
    "    'product_name': 'ITEM_NAME'\n",
    "}, inplace=True)\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(interactions_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71598c08",
   "metadata": {},
   "source": [
    "***Read datasets into S3***\n",
    "\n",
    "Read the interaction,product,and user dataset in the initialized S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read interaction data into S3\n",
    "\n",
    "from io import StringIO\n",
    "interactions_filename = \"interactions.csv\"\n",
    "\n",
    "interactions_df.info()\n",
    "print(interactions_df.columns)\n",
    "\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "interactions_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.put_object(Bucket='personalizeproductreviewdata', Key='interactions.csv', Body = csv_buffer.getvalue())\n",
    "\n",
    "\n",
    "print(interactions_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaca241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read product data into S3\n",
    "\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "product_data.to_csv(csv_buffer, index=False)\n",
    "\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.put_object(Bucket='personalizeproductreviewdata', Key='product_data.csv', Body = csv_buffer.getvalue())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10063884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read user data into S3\n",
    "\n",
    "user_data = user_data.drop('review_content', axis=1)\n",
    "\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "user_data.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.put_object(Bucket='personalizeproductreviewdata', Key='user_data.csv', Body = csv_buffer.getvalue())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
